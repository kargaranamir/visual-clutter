{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9999c05a",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5724fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyrtools as pt\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f20931",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42e0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB2Lab(im):\n",
    "    \"\"\"\n",
    "    \n",
    "    Converts RGB color space to CIELab color space\n",
    "    \n",
    "    Parameters\n",
    "    ---------- \n",
    "    im : an input RGB image\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    im_Lab : the output Lab image\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    im = np.float32(im)/255 # get r,g,b value in the range of [0,1]\n",
    "\n",
    "    # the figure from graybar.m and the infromation from the website \n",
    "    # http://www.cinenet.net/~spitzak/conversion/whysrgb.html, we can conclude\n",
    "    # that our RGB system is sRGB\n",
    "\n",
    "    # if RGB system is sRGB\n",
    "    mask = im >= 0.04045\n",
    "    im[mask] = ((im[mask] + 0.055) / 1.055)**2.4\n",
    "    im[~mask] = im[~mask] / 12.92\n",
    "\n",
    "    # Observer. = 2Â°, Illuminant = D65\n",
    "    matrix = np.array([[0.412453, 0.357580, 0.180423],\n",
    "          [0.212671, 0.715160, 0.072169],\n",
    "          [0.019334, 0.119193, 0.950227]])\n",
    "\n",
    "    c_im = np.dot(im, matrix.T)\n",
    "    c_im[:,:,0] = c_im[:,:,0] / 95.047\n",
    "    c_im[:,:,1] = c_im[:,:,1] / 100.000\n",
    "    c_im[:,:,2] = c_im[:,:,2] / 108.833\n",
    "\n",
    "\n",
    "    mask = c_im >= 0.008856\n",
    "    c_im[mask] = c_im[mask] ** (1/3)\n",
    "    c_im[~mask] = 7.787 * c_im[~mask] + 16/116\n",
    "\n",
    "    im_Lab = np.zeros_like(c_im)\n",
    "\n",
    "    im_Lab[:,:,0] = ( 116 * c_im[:,:,1] ) - 16\n",
    "    im_Lab[:,:,1] = 500 * ( c_im[:,:,0] - c_im[:,:,1] )\n",
    "    im_Lab[:,:,2] = 200 * ( c_im[:,:,1] - c_im[:,:,2] )\n",
    "\n",
    "    \n",
    "    return im_Lab\n",
    "    \n",
    "    \n",
    "def normlize(arr):\n",
    "    \"\"\"\n",
    "    \n",
    "    Normlizes the array input between (min, max) -> (0, 255)\n",
    "    \n",
    "    \"\"\"\n",
    "    return ((arr - arr.min()) * (1/(arr.max() - arr.min()) * 255)).astype('uint8')\n",
    "\n",
    "def conv2(x, y, mode=None):\n",
    "    \n",
    "    if mode == 'same':\n",
    "        return np.rot90(signal.convolve2d(np.rot90(x, 2), np.rot90(y, 2), mode=mode), 2)\n",
    "    else:\n",
    "        return signal.convolve2d(x, y)\n",
    "        \n",
    "\n",
    "def RRoverlapconv(kernel, in_):\n",
    "    \"\"\"\n",
    "    \n",
    "    Filters the image in with filter kernel, where it only \"counts\" the\n",
    "    part of the filter that overlaps the image.  Rescales the filter so its\n",
    "    weights which overlap the image sum to the same as the full filter\n",
    "    kernel.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convolve with the original kernel\n",
    "    out = conv2(in_, kernel, mode='same')\n",
    "    \n",
    "    # Convolve kernel with an image of 1's, of the same size as the input image\n",
    "    rect = np.ones_like(in_)\n",
    "\n",
    "    overlapsum = conv2(rect, kernel, 'same')\n",
    "    # Now scale the output image at each pixel by the relative overlap of the filter with the image\n",
    "    out = np.sum(kernel) * out / overlapsum\n",
    "    return out\n",
    "\n",
    "\n",
    "def RRgaussfilter1D(halfsupport, sigma, center=0):\n",
    "    \"\"\"\n",
    "    \n",
    "    Creates a 1D gaussian filter kernel, centered at center (default=0), with pixels from\n",
    "    a range -halfsupport:halfsupport+1, and standard deviation sigma.\n",
    "    \n",
    "    \"\"\"\n",
    "    t = list(range(-halfsupport, halfsupport+1))\n",
    "    kernel = np.array([np.exp(-(x-center) **2 /(2* sigma ** 2)) for x in t])\n",
    "    kernel = kernel/sum(kernel)\n",
    "    \n",
    "    return kernel.reshape(1,kernel.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "def DoG1filter(a, sigma):\n",
    "    \"\"\"\n",
    "\n",
    "    Creates 2 1-D gaussian filters.\n",
    "    \n",
    "    Parameters\n",
    "    ---------- \n",
    "    a : half-support of the filter. \n",
    "    sigma: standard deviation.  \n",
    "    \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    2-D DoG filters can be contructed by combining 2 1-D DoG filters separably, in x and y directions\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Jitendra Malik and Pietro Perona. Preattentive texture discrimination\n",
    "    with early vision mechanisms. Journal of Optical Society of America A, \n",
    "    7(5), May 1990, 923-932.\n",
    "\n",
    "    Zhenlan Jin\n",
    "    \n",
    "    \"\"\"\n",
    "    sigi = 0.71 * sigma\n",
    "    sigo = 1.14 * sigma\n",
    "    \n",
    "    t = range(-a, a+1)\n",
    "    \n",
    "    gi = [np.exp(-x ** 2 /(2 * sigi ** 2)) for x in t]\n",
    "    gi = gi/sum(gi)\n",
    "    go = [np.exp(- x ** 2/(2 * sigo ** 2)) for x in t]\n",
    "    go = go/sum(go)\n",
    "    \n",
    "    return gi.reshape(1,gi.shape[0]),go.reshape(1,go.shape[0])\n",
    "\n",
    "\n",
    "def addborder(im,xbdr,ybdr,arg):\n",
    "    \"\"\"\n",
    "    \n",
    "    imnew = addborder(im,xborder,yborder,arg)  Make image w/added border.\n",
    "    imnew = addborder(im,5,5,128)  Add 5 wide border of val 128.\n",
    "    imnew = addborder (im,5,5,'even')  Even reflection.\n",
    "    imnew = addborder (im,5,5,'odd')  Odd reflection.\n",
    "    imnew = addborder (im,5,5,'wrap')  Wraparound.\n",
    "    \n",
    "    \"\"\"\n",
    "    ysize, xsize = im.shape\n",
    "    \n",
    "    \n",
    "    # check thickness\n",
    "    if (xbdr > xsize) or (ybdr > ysize):\n",
    "        raise ValueError('borders must be thinner than image')\n",
    "    \n",
    "    # if arg is a number, fill border with its value.\n",
    "    if isinstance(arg, (int, float)):\n",
    "        imbig = cv2.copyMakeBorder(im, ybdr,ybdr,xbdr,xbdr, cv2.BORDER_CONSTANT, value=arg)\n",
    "    \n",
    "    # Even reflections\n",
    "    elif arg == 'even':\n",
    "        imbig = cv2.copyMakeBorder(im, ybdr,ybdr,xbdr,xbdr, cv2.BORDER_REFLECT)\n",
    "        \n",
    "    # Odd reflections\n",
    "    elif arg == 'odd':\n",
    "        imbig = cv2.copyMakeBorder(im, ybdr,ybdr,xbdr,xbdr, cv2.BORDER_REFLECT_101)\n",
    "        \n",
    "    # Wraparound\n",
    "    elif arg == 'wrap':\n",
    "        imbig = cv2.copyMakeBorder(im, ybdr,ybdr,xbdr,xbdr, cv2.BORDER_WRAP)\n",
    "    else:\n",
    "        raise ValueError('unknown border style')\n",
    "    return imbig\n",
    "\n",
    "\n",
    "def filt2(kernel, im1, reflect_style='odd'):\n",
    "    \"\"\"\n",
    "    \n",
    "    Improved version of filter2 in MATLAB, which includes reflection.\n",
    "    Default style is 'odd'. Also can be 'even', or 'wrap'.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    im2 = filt2(kern,image)  apply kernel with odd reflection (default).\n",
    "    im2 = filt2(kern,image,'even')  Use even reflection.\n",
    "    im2 = filt2(kern,image,128)  Fill with 128's.\n",
    "\n",
    "    Ruth Rosenholtz\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ky,kx = kernel.shape\n",
    "    iy,ix = im1.shape\n",
    "\n",
    "    imbig = addborder(im1, kx, ky, reflect_style)\n",
    "    imbig = conv2(imbig, kernel, 'same')\n",
    "    im2 = imbig[ky:ky+iy, kx:kx+ix]\n",
    "\n",
    "\n",
    "    return im2\n",
    "\n",
    "\n",
    "def RRcontrast1channel(pyr, DoG_sigma=2):\n",
    "    \"\"\"\n",
    "    \n",
    "    Filters a Gaussian pyramid, pyr, with a 1-channel contrast feature detector.  \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pyr : a Gaussian pyramid. It can be computed from this \"pyrtools\" package\n",
    "    DoG_sigma : size of the center-surround (Difference-of-Gaussian) filter used for computing the contrast. Default = 2. Refer to DoG1filter.\n",
    "\n",
    "    Code by Ruth Rosenholtz and Zhenlan Jin\n",
    "    modified by Yuanzhen Li, Sep 2004\n",
    "\n",
    "    \"\"\"\n",
    "    levels = len(pyr)\n",
    "    contrast = [0] * levels\n",
    "    \n",
    "    # Here we're using the difference-of-gaussian filters. Separable. \n",
    "    # Refer to routine 'DoG1filter'.\n",
    "    innerG1, outerG1 = DoG1filter(round(DoG_sigma*3), DoG_sigma)\n",
    "\n",
    "    # Do contrast feature computation with these filters:\n",
    "    for i in range(0,levels):\n",
    "        inner = filt2(innerG1, pyr[(i,0)])\n",
    "        inner = filt2(innerG1.T, inner)\n",
    "        outer = filt2(outerG1, pyr[(i,0)])\n",
    "        outer = filt2(outerG1.T, outer)\n",
    "        tmp = inner - outer\n",
    "        contrast[i] = abs(tmp) # ** 2\n",
    " \n",
    "    return contrast\n",
    "\n",
    "\n",
    "def reduce(image0, kernel=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Reduce: for building Gaussian or Laplacian pyramids. 1-D separable kernels.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    imnew = reduce(im0) Reduce w/default kernel: [.05 .25 .4 .25 .05]\n",
    "    imnew = reduce(im0, kern) Reduce with kern; sums to unity.\n",
    "\n",
    "    Ruth Rosenholtz \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if kernel is None:\n",
    "        # Default kernel \n",
    "        kernel = np.array([[0.05, 0.25, 0.4, 0.25, 0.05]])\n",
    "\n",
    "\n",
    "    ysize, xsize = image0.shape\n",
    "\n",
    "    image0 = filt2(kernel,image0) # Filter horizontally. \n",
    "    # filt2 is filter2 with reflection.\n",
    "    image1 = image0[:,range(0,xsize,2)]\n",
    "\n",
    "    image1 = filt2(kernel.T,image1) # Filter vertically.\n",
    "    image2 = image1[range(0,ysize,2),:]\n",
    "\n",
    "    return image2\n",
    "\n",
    "\n",
    "def RRoverlapconvexpand(in_, kernel=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    out = RRoverlapconvexpand(in_)  return an image expanded to double size,\n",
    "    out = RRoverlapconvexpand(in, kernel); specify 1-D kernel with unity sum.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if kernel is None:\n",
    "        # Default kernel \n",
    "        kernel = np.array([[0.05, 0.25, 0.4, 0.25, 0.05]])\n",
    "\n",
    "    ysize, xsize = in_.shape\n",
    "    kernel = kernel * 2 # kernel sum=2 to account for padding.\n",
    "\n",
    "    tmp = np.zeros([ysize,2*xsize]) # First double the width\n",
    "    k = list(range(0, xsize))\n",
    "    k_2 = [x*2 for x in k]\n",
    "    tmp[:,k_2] = in_[:,k]\n",
    "    tmp = RRoverlapconv(kernel,tmp) # ..and filter horizontally. \n",
    "\n",
    "    out = np.zeros([2*ysize,2*xsize]) # Next double the height\n",
    "    k = list(range(0, ysize))\n",
    "    k_2 = [x*2 for x in k]\n",
    "    out[k_2,:] = tmp[k,:]\n",
    "    out = RRoverlapconv(kernel.T,out) # ..and filter vertically.\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def HV(in_):\n",
    "    \"\"\"\n",
    "    \n",
    "    Outputs H-V\n",
    "    \n",
    "    \"\"\"\n",
    "    out = in_[0] - in_[1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def DD(in_):\n",
    "    \"\"\"\n",
    "    \n",
    "    Outputs R-L\n",
    "    \n",
    "    \"\"\"\n",
    "    out = in_[3] - in_[2]\n",
    "    return out\n",
    "\n",
    "\n",
    "def sumorients(in_):\n",
    "    \"\"\"\n",
    "    \n",
    "    Sums the four orientations into one image.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    out =  in_[0] + in_[1] + in_[2] + in_[3]\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def poolnew(in_, sigma=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Pools with a gaussian.  Note assumes that input image is actually\n",
    "    4 equal-size images, side by side.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    in1 = in_[0] #H -> first quarter\n",
    "    in2 = in_[1] #V -> second quarter\n",
    "    in3 = in_[2] #L -> third quarter\n",
    "    in4 = in_[3] #R -> last quarter\n",
    "\n",
    "    \n",
    "    if sigma is None:\n",
    "        out1 = reduce(RRoverlapconvexpand(in1))\n",
    "        out2 = reduce(RRoverlapconvexpand(in2))\n",
    "        out3 = reduce(RRoverlapconvexpand(in3))\n",
    "        out4 = reduce(RRoverlapconvexpand(in4))\n",
    "    else:\n",
    "        kernel = RRgaussfilter1D(round(2*sigma), sigma)\n",
    "        out1 = reduce(RRoverlapconvexpand(in1, kernel), kernel)\n",
    "        out2 = reduce(RRoverlapconvexpand(in2, kernel), kernel)\n",
    "        out3 = reduce(RRoverlapconvexpand(in3, kernel), kernel)\n",
    "        out4 = reduce(RRoverlapconvexpand(in4, kernel), kernel)    \n",
    "\n",
    "    out = out1, out2, out3, out4\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def imrotate(im, angle, method='nearest', bbox='crop'):\n",
    "    \"\"\"\n",
    "    \n",
    "    roatate an image by PIL package\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # interpolation methods\n",
    "    func_method = {'nearest':0,'bilinear':2,'bicubic':3,'cubic':3}\n",
    "    # crop or not methods\n",
    "    func_bbox = {'loose':True,'crop':False}\n",
    "    PIL_im = Image.fromarray(im)\n",
    "    # roatate\n",
    "    im_rot = PIL_im.rotate(angle, expand = func_bbox[bbox], resample = func_method[method])\n",
    "    return np.array(im_rot)\n",
    "\n",
    "def imrotate2(im, angle, method='cubic', bbox='crop'):\n",
    "    \"\"\"\n",
    "    \n",
    "    roatate an image by Scipy package\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # By default rotate uses cubic interpolation\n",
    "    return ndimage.rotate(im, angle=angle)\n",
    "\n",
    "\n",
    "def orient_filtnew(pyr, sigma=16/14):\n",
    "    \"\"\"\n",
    "    \n",
    "    ORIENT_FILTNEW Filters \"pyr\" (in principle, one level of the Gaussian pyramid generated by gausspyr) with 2nd\n",
    "    derivative filters in 4 directions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hvdd : the 4 output images appended together in a list, in the order horizontal, vertical, up-left, and down-right.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    halfsupport = round(3*sigma)   \n",
    "    # halfsupport was 10, for default sigma.  We need a halfsupport of about\n",
    "    # 2*sigma for a single Gaussian.  Here we have three, one at -sigma, one at\n",
    "    # sigma, so we should need a halfsupport of about 3*sigma.\n",
    "\n",
    "\n",
    "    sigy = sigma\n",
    "    sigx = sigma # Was sigx = 3*sigma.\n",
    "\n",
    "    gx = RRgaussfilter1D(halfsupport, sigx)\n",
    "    gy = RRgaussfilter1D(halfsupport, sigy, sigma)\n",
    "    Ga = conv2(gx, gy.T)\n",
    "    Ga = Ga/sum(sum(Ga))\n",
    "    gy = RRgaussfilter1D(halfsupport, sigy)\n",
    "    Gb = conv2(gx, gy.T)\n",
    "    Gb = Gb/sum(sum(Gb))\n",
    "    gy = RRgaussfilter1D(halfsupport, sigy, -sigma)\n",
    "    Gc = conv2(gx, gy.T)\n",
    "    Gc = Gc/sum(sum(Gc))\n",
    "    H = -Ga+2*Gb-Gc\n",
    "    V = H.T\n",
    "\n",
    "\n",
    "    GGa = imrotate2(Ga, 45, 'bicubic', 'crop')\n",
    "    GGa = GGa/sum(sum(GGa))\n",
    "    GGb = imrotate2(Gb, 45, 'bicubic', 'crop')\n",
    "    GGb = GGb/sum(sum(GGb))\n",
    "    GGc = imrotate2(Gc, 45, 'bicubic', 'crop')\n",
    "    GGc = GGc/sum(sum(GGc))\n",
    "    R = -GGa+2*GGb-GGc\n",
    "    GGa = imrotate2(Ga, -45, 'bicubic', 'crop')\n",
    "    GGa = GGa/sum(sum(GGa))\n",
    "    GGb = imrotate2(Gb, -45, 'bicubic', 'crop')\n",
    "    GGb = GGb/sum(sum(GGb))\n",
    "    GGc = imrotate2(Gc, -45, 'bicubic', 'crop')\n",
    "    GGc = GGc/sum(sum(GGc))\n",
    "    L = -GGa+2*GGb-GGc\n",
    "\n",
    "    hout = filt2(H,pyr)\n",
    "    vout = filt2(V,pyr)\n",
    "    lout = filt2(L,pyr)\n",
    "    rout = filt2(R,pyr)\n",
    "\n",
    "    hvdd = hout, vout, lout, rout\n",
    "\n",
    "    return hvdd\n",
    "\n",
    "\n",
    "def entropy(x, nbins=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Computes the entropy of signal \"x\", given the number of bins \"nbins\" used uniform binning in the calculation.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    nsamples = x.shape[0]\n",
    "    \n",
    "    if nbins is None:\n",
    "        nbins = int(np.ceil(np.sqrt(nsamples)))\n",
    "\n",
    "    ref_range = (x.min(), x.max())\n",
    "    ref_hist, _ = np.histogram(x, bins=nbins, range=ref_range)\n",
    "    \n",
    "    ref_hist = ref_hist / float(np.sum(ref_hist))\n",
    "    ref_hist = ref_hist[np.nonzero(ref_hist)]\n",
    "    ref_ent = -np.sum(ref_hist * np.log(ref_hist))\n",
    "\n",
    "    return ref_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f082b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clutter():\n",
    "    \"\"\"\n",
    "    \n",
    "    Class of two measures of visual clutter (Feature Congestion and Subband Entropy)\n",
    "    \n",
    "    Parameters\n",
    "    ---------- \n",
    "    inputImage : gives the input. It can be one of the following 2 things: 1. an RGB image; 2. a string, i.e., file name of an RGB image.\n",
    "    numlevels : the number of levels.\n",
    "    contrast_filt_sigma : the sigma (standard deviation) of the center-surround DoG1 filter used for computing the contrast \n",
    "    contrast_pool_sigma : the sigma (standard deviation) of this Gaussian window for contrast clutter. Default = 3*filt_sigma. \n",
    "    color_pool_sigma : the sigma (standard deviation) of this Gaussian window for color clutter, Defaults to 3.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    getClutter_FC: computes Feature Congestion clutter, outputs both a scalar (clutter of the whole image) and a map (local clutter).\n",
    "    getClutter_SE: computes Subband Entropy clutter, outputs only a scalar.\n",
    "    colorClutter: computes clutter maps indicating local variability in color\n",
    "    contrastClutter: computes clutter maps indicating local variability in contrast\n",
    "    orientationClutter: computes clutter maps indicating local variability in orientation\n",
    " \n",
    "    (Please see individual routines for more info about parameters and outputs.)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Ruth Rosenholtz, Yuanzhen Li, and Lisa Nakano. \"Measuring Visual Clutter\". \n",
    "    Journal of Vision, 7(2), 2007. http://www.journalofvision.com/7/2/\n",
    "    Ruth Rosenholtz, Yuanzhen Li, and Lisa Nakano, March 2007.\n",
    "    \n",
    "    Ruth Rosenholtz, Yuanzhen Li, Jonathan Mansfield, and Zhenlan Jin. \"Feature Congestion: A Measure of Display Clutter\".\n",
    "    CHI '05: Proc. of the SIGCHI conference on Human factors in computing systems. May 2005. 761-770. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputImage, numlevels=3, contrast_filt_sigma=1, contrast_pool_sigma=None, color_pool_sigma=3):\n",
    "        self.inputImage = inputImage\n",
    "        self.numlevels = numlevels\n",
    "        self.contrast_filt_sigma = contrast_filt_sigma\n",
    "        self.contrast_pool_sigma = 3 * contrast_filt_sigma if contrast_pool_sigma is None else contrast_pool_sigma\n",
    "        self.color_pool_sigma = color_pool_sigma\n",
    "        # orient_pool_sigma is the sigma (standard deviation) of this Gaussian window, and here is hard-wired to 7/2.\n",
    "        self.orient_pool_sigma = 7/2\n",
    "        \n",
    "        if isinstance(inputImage, str): \n",
    "            self.im = cv2.imread(inputImage)\n",
    "            if self.im is None:\n",
    "                raise ValueError(f'Unable to open {inputImage} image file.')     \n",
    "            self.im = cv2.cvtColor(self.im, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        elif isinstance(inputImage,np.ndarray):\n",
    "            self.im = inputImage\n",
    "        \n",
    "\n",
    "        # The input image, im, had better be a MxNx3 matrix, in which case we assume it is an RGB image.\n",
    "        # If it's MxN, it's probably gray, and color clutter method is not appropriate.        \n",
    "        number_of_dimension = len(self.im.shape)\n",
    "        if number_of_dimension == 3: \n",
    "            self.m, self.n, self.d = self.im.shape\n",
    "        elif number_of_dimension == 2: \n",
    "            self.m, self.n = self.im.shape\n",
    "            self.d = 1\n",
    "        else: \n",
    "            raise ValueError(f'inputImage should be as one of these formats: MxNxD or MxN')    \n",
    "             \n",
    "        if self.d == 3:\n",
    "            # we first convert it into the perceptually-based CIELab color space.\n",
    "            self.Lab = RGB2Lab(self.im)\n",
    "            Lab_float = self.Lab.astype(np.float32)\n",
    "            # luminance(L) and the chrominance(a,b) channels\n",
    "            self.L, self.a, self.b = cv2.split(Lab_float)\n",
    "\n",
    "            # Get Gaussian pyramids (one for each of L,a,b)\n",
    "            pyr = pt.pyramids.GaussianPyramid(self.L, height=self.numlevels)\n",
    "            self.L_pyr = pyr.pyr_coeffs\n",
    "            pyr = pt.pyramids.GaussianPyramid(self.a, height=self.numlevels)\n",
    "            self.a_pyr = pyr.pyr_coeffs\n",
    "            pyr = pt.pyramids.GaussianPyramid(self.b, height=self.numlevels)\n",
    "            self.b_pyr = pyr.pyr_coeffs\n",
    "            self.RRLab = [self.L_pyr, self.a_pyr, self.b_pyr] \n",
    "\n",
    "        else: \n",
    "            self.L = self.im  \n",
    "            pyr = pt.pyramids.GaussianPyramid(L, height=numlevels)\n",
    "            self.L_pyr = pyr.pyr_coeffs\n",
    "            print ('Input image appears to be grayscale, so you can only use contrast clutter method\\n')\n",
    "            \n",
    "    \n",
    "    \n",
    "    def collapse(self, clutter_levels):\n",
    "        \"\"\"\n",
    "        \n",
    "        Collapse over scales by taking the maximum.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        first get a Gaussian kernel to upsample the clutter maps on bigger scales\n",
    "        so that the clutter maps would have the same sizes, and max can be taken\n",
    "        across scales.\n",
    "        \n",
    "        \"\"\"\n",
    "        kernel_1d = np.array([[0.05, 0.25, 0.4, 0.25, 0.05]])\n",
    "        kernel_2d = conv2(kernel_1d, kernel_1d.T)\n",
    "\n",
    "        clutter_map = clutter_levels[0]\n",
    "        for scale in range(1,len(clutter_levels)):\n",
    "            clutter_here = clutter_levels[scale]\n",
    "\n",
    "            for kk in range(scale, 0, -1):\n",
    "                clutter_here = pt.upConv(image=clutter_here, filt=kernel_2d, edge_type='reflect1', step=[2,2], start=[0,0])\n",
    "\n",
    "            common_sz = min(clutter_map.shape[0], clutter_here.shape[0]), min(clutter_map.shape[1], clutter_here.shape[1])\n",
    "            for i in range(0, common_sz[0]):\n",
    "                for j in range(0, common_sz[1]):\n",
    "                     clutter_map[i][j] = max(clutter_map[i][j], clutter_here[i][j])\n",
    "\n",
    "    \n",
    "        return clutter_map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def display(self, method=''):\n",
    "        \"\"\"\n",
    "        \n",
    "        Saves the clutter map(s)\n",
    "        \n",
    "        \"\"\"\n",
    "        if method == 'color':\n",
    "            clutter_map = self.color_clutter_map\n",
    "            clutter_levels = self.color_clutter_levels\n",
    "            \n",
    "        elif method == 'contrast':\n",
    "            clutter_map = self.contrast_clutter_map\n",
    "            clutter_levels = self.contrast_clutter_levels\n",
    "\n",
    "        elif method == 'orientation':\n",
    "            clutter_map = self.orientation_clutter_map\n",
    "            clutter_levels = self.orientation_clutter_levels\n",
    "        \n",
    "        elif method == 'combine':\n",
    "            clutter_levels = None\n",
    "            clutter_map = self.clutter_map_fc\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"method is not given or incorrect, should be selected from this list: ['color','contrast','orientation', 'combine']\")\n",
    "            \n",
    "        min_min = np.mean(clutter_map)\n",
    "        max_max = np.max(clutter_map)\n",
    "\n",
    "        if clutter_levels is not None:\n",
    "            numlevels = len(clutter_levels)\n",
    "            size = clutter_map.shape[::-1]\n",
    "            if numlevels > 8:\n",
    "                raise ValueError('too many levels!!')\n",
    "\n",
    "\n",
    "            for scale in range(numlevels):\n",
    "                arr = clutter_levels[scale]\n",
    "                new_arr = normlize(arr)\n",
    "                new_PIL = Image.fromarray(new_arr)\n",
    "                new_PIL = new_PIL.resize(size, Image.ANTIALIAS)\n",
    "                \n",
    "                # save clutter level(s) \n",
    "                new_PIL.save(f'{method} at level {scale}.png')\n",
    "\n",
    "        new_arr = normlize(clutter_map)\n",
    "        new_PIL = Image.fromarray(new_arr)\n",
    "        \n",
    "        # save collapsed clutter map(s)\n",
    "        new_PIL.save(f'collapsed {method} map.png')\n",
    "\n",
    "\n",
    "    def computeColorClutter(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        Computes the color clutter maps. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        color_clutter_levels : a list structure, containing the color clutter at a \n",
    "        number of scales specified by numlevels.\n",
    "        the n'th level of which can be accessed using color_clutter_levels[n], n starts from 0 to numlevels-1\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Color clutter is computed as the \"volume\" of a color distribution\n",
    "        ellipsoid, which is the determinant of covariance matrix. Covariance \n",
    "        matrix can be computed efficiently through linear filtering. More \n",
    "        specifically, cov(X,Y) = E(XY)-E(X)E(Y), where E (expectation value) \n",
    "        can be approximated by filtering with a Gaussian window. \n",
    "        \"\"\"\n",
    "        \n",
    "        # initiatialization\n",
    "        covMx = {}\n",
    "        self.color_clutter_levels = [0] * self.numlevels\n",
    "        DL = [0] * self.numlevels\n",
    "        Da = [0] * self.numlevels\n",
    "        Db = [0] * self.numlevels\n",
    "\n",
    "\n",
    "        # sensitivitis to the L,a,and b channels are different, therefore we use\n",
    "        # deltaL2, deltaa2, and deltab2 to \"scale\" the L,a,b axes when computing\n",
    "        # the covariance matrix. Eventually these numbers should be vary according\n",
    "        # to the spatial scales, mimicing our visual system's sensitivity function\n",
    "        deltaL2 = 0.0007 ** 2\n",
    "        deltaa2 = 0.1 ** 2\n",
    "        deltab2 = 0.05 ** 2\n",
    "\n",
    "        # Get a Gaussian filter for computing the covariance\n",
    "        bigG = RRgaussfilter1D(round(2*self.color_pool_sigma), self.color_pool_sigma)\n",
    "\n",
    "        for i in range(0, self.numlevels):\n",
    "            # get E(X) by filtering X with a 1-D Gaussian window separably in x and y directions:\n",
    "            DL[i] = RRoverlapconv(bigG, self.L_pyr[(i,0)])\n",
    "            DL[i] = RRoverlapconv(bigG.T, DL[i])   # E(L)\n",
    "            Da[i] = RRoverlapconv(bigG, self.a_pyr[(i,0)])\n",
    "            Da[i] = RRoverlapconv(bigG.T, Da[i])   # E(a)\n",
    "            Db[i] = RRoverlapconv(bigG, self.b_pyr[(i,0)]);\n",
    "            Db[i] = RRoverlapconv(bigG.T, Db[i])    # E(b)\n",
    "\n",
    "\n",
    "            # Covariance matrix \n",
    "            # covMx(L,a,b) = | cov(L,L)  cov(L,a)  cov(L,b) |\n",
    "            #                | cov(a,L)  cov(a,a)  cov(a,b) |\n",
    "            #                | cov(b,L)  cov(b,a)  cov(b,b) |\n",
    "            # where cov(X,Y) = E(XY) - E(X)E(Y)\n",
    "            #   and if X is the same as Y, then it's the variance var(X) =\n",
    "            #   E(X.^2)-E(X).^2\n",
    "            # and as cov(X,Y) = cov(Y,X), covMx is symmetric\n",
    "            # covariance matrix elements:\n",
    "            covMx[(i,0,0)] = RRoverlapconv(bigG, self.L_pyr[(i,0)] ** 2)\n",
    "            covMx[(i,0,0)] = RRoverlapconv(bigG.T, covMx[(i,0,0)]) - DL[i] ** 2 + deltaL2  # cov(L,L) + deltaL2\n",
    "            covMx[(i,0,1)] = RRoverlapconv(bigG, self.L_pyr[(i,0)] * self.a_pyr[(i,0)])\n",
    "            covMx[(i,0,1)] = RRoverlapconv(bigG.T, covMx[(i,0,1)]) - DL[i] * Da[i]        # cov(L,a)\n",
    "            covMx[(i,0,2)] = RRoverlapconv(bigG, self.L_pyr[(i,0)] * self.b_pyr[(i,0)])\n",
    "            covMx[(i,0,2)] = RRoverlapconv(bigG.T, covMx[(i,0,2)]) - DL[i] * Db[i]        # cov(L,b)\n",
    "            covMx[(i,1,1)] = RRoverlapconv(bigG, self.a_pyr[(i,0)] ** 2)\n",
    "            covMx[(i,1,1)] = RRoverlapconv(bigG.T, covMx[(i,1,1)]) - Da[i] ** 2 + deltaa2  # cov(a,a) + deltaa2\n",
    "            covMx[(i,1,2)] = RRoverlapconv(bigG, self.a_pyr[(i,0)] * self.b_pyr[(i,0)])\n",
    "            covMx[(i,1,2)] = RRoverlapconv(bigG.T, covMx[(i,1,2)]) - Da[i] * Db[i]        # cov(a,b)\n",
    "            covMx[(i,2,2)] = RRoverlapconv(bigG, self.b_pyr[(i,0)] ** 2)    \n",
    "            covMx[(i,2,2)] = RRoverlapconv(bigG.T, covMx[(i,2,2)]) - Db[i] ** 2 + deltab2;  # cov(b,b) + deltab2\n",
    "\n",
    "            # Get the determinant of covariance matrix\n",
    "            # which is the \"volume\" of the covariance ellipsoid\n",
    "            detIm = covMx[(i,0,0)]*(covMx[(i,1,1)]*covMx[(i,2,2)]-covMx[(i,1,2)]*covMx[(i,1,2)])\\\n",
    "            - covMx[(i,0,1)]*(covMx[(i,0,1)]*covMx[(i,2,2)]-covMx[(i,1,2)]*covMx[(i,0,2)])\\\n",
    "            + covMx[(i,0,2)]*(covMx[(i,0,1)]*covMx[(i,1,2)]-covMx[(i,1,1)]*covMx[(i,0,2)])\n",
    "\n",
    "            # take the square root considering variance is squared, and the cube\n",
    "            # root, since this is the volume and the contrast measure is a \"length\"\n",
    "            self.color_clutter_levels[i] = np.sqrt(detIm) ** (1/3)\n",
    "        return self.color_clutter_levels\n",
    "    \n",
    "\n",
    "    def colorClutter(self, color_pix=0):\n",
    "        \"\"\"\n",
    "        \n",
    "        Computes the color clutter map(s) of an image. \n",
    "         \n",
    "        Parameters\n",
    "        ---------- \n",
    "        color_pix : if it is 1 then saves the color clutter map(s) as png files  If it's 0, does not save\n",
    "         (useful for batch processing of many images).  Defaults not to save.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        color_clutter_levels : a list structure, containing the color clutter at a number \n",
    "        of scales specified by numlevels; the n'th level of which can be accessed using color_clutter_levels[n], n starts from 0 to numlevels-1\n",
    "        color_clutter_map : an array of the same size as inputImage, is a single clutter map \n",
    "        collapsed from color_clutter_levels, which is the clutter measure at multiple scales\n",
    "        now the \"collapsing\" is done by taking the maximal values across scales\n",
    " \n",
    "        Notes\n",
    "        -----\n",
    "        Color clutter is computed as the \"volume\" of a color distribution\n",
    "        ellipsoid, which is the determinant of covariance matrix. Covariance \n",
    "        matrix can be computed efficiently through linear filtering. More \n",
    "        specifically, cov(X,Y) = E(XY)-E(X)E(Y), where E (expectation value) \n",
    "        can be approximated by filtering with a Gaussian window. \n",
    "        \n",
    "        \"\"\"\n",
    "        self.color_pix = color_pix\n",
    "        \n",
    "        # Compute clutter\n",
    "        self.color_clutter_levels = self.computeColorClutter()\n",
    "        self.color_clutter_map = self.collapse(self.color_clutter_levels)\n",
    "        \n",
    "        # to save the clutter maps:       \n",
    "        if self.color_pix==1:\n",
    "            self.display(method='color')\n",
    "\n",
    "        return self.color_clutter_levels, self.color_clutter_map\n",
    "\n",
    "    \n",
    "    def contrastClutter(self, contrast_pix=0):  \n",
    "        \"\"\"\n",
    "        \n",
    "        Computes the contrast clutter map(s) of an image.\n",
    "        \n",
    "        Parameters\n",
    "        ---------- \n",
    "        contrast_pix : if it is 1 then saves the contrast clutter map(s) as png files  If it's 0, does not save\n",
    "         (useful for batch processing of many images).  Defaults to not save.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        contrast_clutter_levels : a list structure, containing the orientation clutter at a number \n",
    "        of scales specified by numlevels; the n'th level of which can be accessed using contrast_clutter_levels[n], n starts from 0 to numlevels-1\n",
    "        contrast_clutter_map : an array of the same size as inputImage, is a single clutter map \n",
    "        collapsed from contrast_clutter_levels, which is the clutter measure at multiple scales\n",
    "        now the \"collapsing\" is done by taking the maximal values across scales\n",
    " \n",
    "        \"\"\"\n",
    "        self.contrast_pix = contrast_pix\n",
    "        # We then compute a form of \"contrast-energy\" by filtering the luminance\n",
    "        # channel L by a center-surround filter and squaring (or taking the absolute \n",
    "        # values of) the filter outputs. The center-surround filter is a DoG1 filter \n",
    "        # with std 'contrast_filt_sigma'.\n",
    "        contrast = RRcontrast1channel(self.L_pyr, self.contrast_filt_sigma)\n",
    "\n",
    "        # initiate clutter_map and clutter_levels:\n",
    "        m, n = len(contrast), 1\n",
    "        self.contrast_clutter_levels = [0] * m\n",
    "        \n",
    "        # Get a Gaussian filter for computing the variance of contrast\n",
    "        # Since we used a Gaussian pyramid to find contrast features, these filters \n",
    "        # have the same size regardless of the scale of processing.\n",
    "        bigG = RRgaussfilter1D(round(self.color_pool_sigma*2), self.color_pool_sigma)\n",
    "\n",
    "        for scale in range(0,m):\n",
    "            for channel in range(0,n):\n",
    "                # var(X) = E(X.^2) - E(X).^2\n",
    "                # get E(X) by filtering X with a 1-D Gaussian window separably in x and y directions\n",
    "                meanD = RRoverlapconv(bigG, contrast[scale])\n",
    "                meanD = RRoverlapconv(bigG.T, meanD)\n",
    "                # get E(X.^2) by filtering X.^2 with a 1-D Gaussian window separably in x and y directions\n",
    "                meanD2 = RRoverlapconv(bigG, contrast[scale] ** 2)\n",
    "                meanD2 = RRoverlapconv(bigG.T, meanD2)\n",
    "\n",
    "                # get variance by var(X) = E(X.^2) - E(X).^2\n",
    "                stddevD = np.sqrt(abs(meanD2 - meanD ** 2))\n",
    "                self.contrast_clutter_levels[scale] = stddevD\n",
    "\n",
    "        self.contrast_clutter_map = self.collapse(self.contrast_clutter_levels)\n",
    "        \n",
    "        \n",
    "        if self.contrast_pix==1:\n",
    "            self.display(method='contrast')\n",
    "\n",
    "        return self.contrast_clutter_levels, self.contrast_clutter_map\n",
    "    \n",
    "\n",
    "    def RROrientationOppEnergy(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        OPP_ENERGY    This runs the oriented opponent energy calculation that\n",
    "        serves as the first stages in Bergen & Landy's (1990)\n",
    "        texture segmentor, except it uses DOOG filters (which actually\n",
    "        don't work as well, but at least we can more easily control the\n",
    "        scale).\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        hvdd = [0] * self.numlevels\n",
    "        hv = [0] * self.numlevels\n",
    "        dd = [0] * self.numlevels\n",
    "        out = [0] * self.numlevels\n",
    "        total = [0] * self.numlevels\n",
    "\n",
    "        noise = 1.0    # Was 1.5\n",
    "        filterScale = 16/14*1.75\n",
    "        poolScale = 1.75\n",
    "        # These probably seem like arbitrary numbers, but it's just trying to get\n",
    "        # three very different feature extraction methods to operate at basically\n",
    "        # the same scales.\n",
    "\n",
    "\n",
    "        for scale in range(0, self.numlevels):\n",
    "            # Check this is the right order for Landy/Bergen. RRR\n",
    "            hvdd[scale] = orient_filtnew(self.L_pyr[(scale,0)],filterScale) \n",
    "            # filt with 4 oriented filters 0, 45, 90, 135.  Was sigma = 16/14, orient_filtnew,\n",
    "            # then 16/14*1.75 to match contrast and other scales.\n",
    "            # Eventually make this sigma a variable that's passed to this routine.\n",
    "            # hvdd[scale] is the 4 output images concatenated together, \n",
    "            # in the order horizontal, vertical, up-left, and down-right.\n",
    "\n",
    "            hvdd[scale] = [x ** 2 for x in hvdd[scale]]    #local energy\n",
    "            hvdd[scale] = poolnew(hvdd[scale], poolScale) #Pools with a gaussian filter.  Was effectively sigma=1, then 1.75 to match 1.75 above.\n",
    "            # RRR Should look at these results and see if this is the right amount of\n",
    "            # pooling for the new filters.  It was right for the Landy-Bergen\n",
    "            # filters.\n",
    "            hv[scale] = HV(hvdd[scale]) # get the difference image between horizontal and vertical: H-V (0-90)\n",
    "            dd[scale] = DD(hvdd[scale]) # get the difference image between right and left: R-L (45-135)\n",
    "            # Normalize by the total response at this scale, assuming the total\n",
    "            # response is high enough.  If it's too low, we'll never see this\n",
    "            # orientation.  I'm not sure what to do here -- set it to zeros and\n",
    "            # it's like that's the orientation.  Maybe output the total response\n",
    "            # and decide what to do later.  RRR\n",
    "            total[scale] = sumorients(hvdd[scale]) + noise # add noise based upon sumorients at visibility threshold\n",
    "            hv[scale] = hv[scale]/total[scale] # normalize the hv and dd image\n",
    "            dd[scale] = dd[scale]/total[scale]\n",
    "            out[scale] = hv[scale], dd[scale] # out is the 2 output images concatenated together, in the order of hv, dd\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def computeOrientationClutter(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        Computes the orientation clutter maps. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        orientation_clutter_levels : a list structure, containing the orientation clutter at a \n",
    "        number of scales specified by numlevels.\n",
    "        the n'th level of which can be accessed using orientation_clutter_levels[n], n starts from 0 to numlevels-1\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Orientation clutter is computed as the \"volume\" of an orientation distribution\n",
    "        ellipsoid, which is the determinant of covariance matrix. Treats cos(2 theta)\n",
    "        and sin(2 theta) (computed from OrientedOppEnergy) as a two-vector, and gets\n",
    "        The covariance of this two-vector.  The covariance \n",
    "        matrix can be computed efficiently through linear filtering. More \n",
    "        specifically, cov(X,Y) = E(XY)-E(X)E(Y), where E (expectation value) \n",
    "        can be approximated by filtering with a Gaussian window. \n",
    "        poolScale is set to 7/2.\n",
    "\n",
    "        This currently seems far too dependent on luminance contrast.  Check into\n",
    "        why this is so -- I thought we were normalizing by local contrast.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        noise = 0.001  # Was eps, but that gave too much orientation noise in the saliency maps.  Then changed to 0.000001\n",
    "        poolScale = 7/2\n",
    "\n",
    "        numlevels = len(self.L_pyr);\n",
    "        Dc = [0] * numlevels  # mean \"cos 2 theta\" at distractor scale\n",
    "        Ds = [0] * numlevels  # mean \"sin 2 theta\" at distractor scale\n",
    "\n",
    "        # Get approximations to cos(2theta) and sin(2theta) from oriented opponent\n",
    "        # energy, at each of the numlevels of the pyramid\n",
    "        angles = self.RROrientationOppEnergy()\n",
    "\n",
    "        # Compute the two-vector [meancos, meansin] at each scale, as well as the\n",
    "        # things we need to compute the mean and covariance of this two-vector at\n",
    "        # the larger, distractor scale.\n",
    "        bigG = RRgaussfilter1D(round(8*poolScale), 4*poolScale)\n",
    "        maxbigG = max(bigG) ** 2\n",
    "\n",
    "\n",
    "        covMx = {}\n",
    "        self.orientation_clutter_levels = [0] * numlevels\n",
    "\n",
    "        for i in range(0,numlevels):\n",
    "            cmx = angles[i][0]\n",
    "            smx = angles[i][1]\n",
    "\n",
    "            # Pool to get means at distractor scale. In pooling, don't pool over the target\n",
    "            # region (implement this by pooling with a big Gaussian, then\n",
    "            # subtracting the pooling over the target region computed above.  Note,\n",
    "            # however, that we first need to scale the target region pooling so\n",
    "            # that its peak is the same height as this much broader Gaussian used\n",
    "            # to pool over the distractor region.\n",
    "            Dc[i] = RRoverlapconv(bigG, cmx)\n",
    "            Dc[i] = RRoverlapconv(bigG.T, Dc[i])\n",
    "            Ds[i] = RRoverlapconv(bigG, smx)\n",
    "            Ds[i] = RRoverlapconv(bigG.T, Ds[i])\n",
    "\n",
    "            # Covariance matrix elements.  Compare with computations in\n",
    "            # RRStatisticalSaliency.  I tried to match computeColorClutter, but I\n",
    "            # don't remember the meaning of some of the terms I removed.  XXX\n",
    "            covMx[(i,0,0)] = RRoverlapconv(bigG, cmx ** 2)\n",
    "            covMx[(i,0,0)] = RRoverlapconv(bigG.T, covMx[(i,0,0)]) - Dc[i] ** 2 + noise\n",
    "            covMx[(i,0,1)] = RRoverlapconv(bigG, cmx * smx)\n",
    "            covMx[(i,0,1)] = RRoverlapconv(bigG.T, covMx[(i,0,1)]) - Dc[i] * Ds[i]\n",
    "            covMx[(i,1,1)] = RRoverlapconv(bigG, smx ** 2)\n",
    "            covMx[(i,1,1)] = RRoverlapconv(bigG.T, covMx[(i,1,1)]) - Ds[i] ** 2 + noise\n",
    "\n",
    "            # Get determinant of covariance matrix, which is the volume of the\n",
    "            # covariance ellipse\n",
    "            detIm = covMx[(i,0,0)] * covMx[(i,1,1)] - covMx[(i,0,1)] ** 2\n",
    "            # Take the square root considering variance is squared, and the square\n",
    "            # root again, since this is the area and the contrast measure is a \"length\"\n",
    "            self.orientation_clutter_levels[i] = detIm ** (1/4)\n",
    "\n",
    "        return self.orientation_clutter_levels\n",
    "    \n",
    "\n",
    "    def orientationClutter(self, orient_pix=0):\n",
    "        \"\"\"\n",
    "        Computes the orientation clutter map(s) of an image. \n",
    "         \n",
    "        Parameters\n",
    "        ---------- \n",
    "        orient_pix : if it is 1 then saves the orientation clutter map(s) as png files  If it's 0, does not save\n",
    "         (useful for batch processing of many images).  Defaults not to save.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        orientation_clutter_levels : a list structure, containing the orientation clutter at a number \n",
    "        of scales specified by numlevels; the n'th level of which can be accessed using orientation_clutter_levels[n], n starts from 0 to numlevels-1\n",
    "        orientation_clutter_map : an array of the same size as inputImage, is a single clutter map \n",
    "        collapsed from orientation_clutter_levels, which is the clutter measure at multiple scales\n",
    "        now the \"collapsing\" is done by taking the maximal values across scales\n",
    " \n",
    "        Notes\n",
    "        -----\n",
    "        Orientation clutter is computed as the \"volume\" of an orientation distribution\n",
    "        ellipsoid, which is the determinant of covariance matrix. Covariance \n",
    "        matrix can be computed efficiently through linear filtering. More \n",
    "        specifically, cov(X,Y) = E(XY)-E(X)E(Y), where E (expectation value) \n",
    "        can be approximated by filtering with a Gaussian window. \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.orient_pix = orient_pix\n",
    "        #  Compute clutter\n",
    "        self.orientation_clutter_levels = self.computeOrientationClutter()\n",
    "        self.orientation_clutter_map = self.collapse(self.orientation_clutter_levels)\n",
    "        \n",
    "        # to save the clutter maps:\n",
    "        if self.orient_pix==1:\n",
    "            self.display(method='orientation')\n",
    "\n",
    "        return self.orientation_clutter_levels, self.orientation_clutter_map\n",
    "    \n",
    "    def computeClutter(self, color_pix=0, contrast_pix=0, orient_pix=0) -> tuple:\n",
    "        \"\"\"        \n",
    "        \n",
    "        Computes Feature Congestion clutter map(s) of an image.\n",
    "\n",
    "        Parameters\n",
    "        ---------- \n",
    "        color_pix :if it is 1 then saves the color clutter map(s) as png files\n",
    "        contrast_pix : if it is 1 then saves the contrast clutter map(s) as png files\n",
    "        orient_pix : if it is 1 then saves the orientation clutter map(s) as png files\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        color_clutter : a list structure containing the color clutter map(s)\n",
    "        contrast_clutter: a list structure containing the contrast clutter map(s)\n",
    "        orientation_clutter: a list structure containing the orientation clutter map(s)\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        As for each of the three list structure, *[0] is another list structure containing \n",
    "        the clutter maps at a number of scales specified by \"numlevels\", and *[1] is a \n",
    "        single clutter map (same size as the input image) collapsed from all scales\n",
    "  \n",
    "        Examples\n",
    "        --------\n",
    "        >>> clt = Clutter('test.jpg', numlevels=3, contrast_filt_sigma=1, contrast_pool_sigma=3, color_pool_sigma=3)\n",
    "        >>> color_clutter, contrast_clutter, orientation_clutter = clt.computeClutter(color_pix=1, contrast_pix=1, orient_pix=1)\n",
    "  \n",
    "        \"\"\"\n",
    "\n",
    "        # compute the color clutter\n",
    "        color_clutter_levels, color_clutter_map = self.colorClutter(color_pix = color_pix)\n",
    "        # compute the contrast clutter\n",
    "        contrast_clutter_levels, contrast_clutter_map = self.contrastClutter(contrast_pix = contrast_pix)\n",
    "        # compute the orientation clutter\n",
    "        orient_clutter_levels, orientation_clutter_map = self.orientationClutter(orient_pix = orient_pix)\n",
    "\n",
    "        # output them in list structures\n",
    "        color_clutter = [color_clutter_levels, color_clutter_map]\n",
    "        contrast_clutter = [contrast_clutter_levels, contrast_clutter_map]\n",
    "        orientation_clutter = [orient_clutter_levels, orientation_clutter_map]\n",
    "\n",
    "        return color_clutter, contrast_clutter, orientation_clutter\n",
    "    \n",
    "    \n",
    "    def getClutter_FC(self, p=1, pix=0) -> (float, np.array):\n",
    "        \"\"\"\n",
    "\n",
    "        Computes Feature Congestion measure of visual clutter.\n",
    "        \n",
    "        Parameters\n",
    "        ---------- \n",
    "        p : a parameter when combining local clutter over \n",
    "        space; the combination can be considered Minkowski distance of order p\n",
    "        pix : if it is 1 then saves the outputs as png files\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        clutter_scalar_fc : is a scalar, which gives the Feature Congestion clutter of the whole image.\n",
    "        clutter_map_fc : is a clutter map (same size as the input image), which gives local clutter information.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This measure (Feature Congestion) of visual clutter is related to the\n",
    "        local variability in certain key features, e.g., color, contrast, and orientation.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> clt = Clutter('test.jpg', numlevels=3, contrast_filt_sigma=1, contrast_pool_sigma=3, color_pool_sigma=3)\n",
    "        >>> clutter_scalar_fc, clutter_map_fc = clt.getClutter_FC(p=1, pix=1)\n",
    "        \n",
    "        \"\"\"\n",
    "        color_clutter, contrast_clutter, orient_clutter = self.computeClutter(pix, pix, pix)\n",
    "        self.clutter_map_fc = color_clutter[1] / 0.2088 + contrast_clutter[1] / 0.0660 + orient_clutter[1] / 0.0269\n",
    "        self.clutter_scalar_fc = np.mean(self.clutter_map_fc ** p) ** (1 / p) #element wise\n",
    "        \n",
    "        if pix==1:\n",
    "            self.display(method='combine')\n",
    "        return self.clutter_scalar_fc, self.clutter_map_fc\n",
    "\n",
    "\n",
    "    def band_entropy(self, map_, wlevels, wor) -> list:\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ---------- \n",
    "        map_ : a monochromatic image\n",
    "        wlevels : the number of spatial scales for the subband decomposition\n",
    "        wor : the number of orientations for the subband decomposition\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        en_band : a vector containing Shannon entropies of all the subbands\n",
    "        \n",
    "        Examples\n",
    "        --------\n",
    "        # luminance channel:\n",
    "        >>> clt = Clutter('test.jpg')\n",
    "        >>> l_channel = clt.L\n",
    "        >>> en_band = clt.band_entropy(l_channel, wlevels=3, wor=4)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # decompose the image into subbands:\n",
    "        self.SFpyr = pt.pyramids.SteerablePyramidFreq(map_, height=wlevels, order=wor-1)\n",
    "        S = self.SFpyr.pyr_coeffs\n",
    "        \n",
    "        en_band = []\n",
    "        for ind in S.keys():\n",
    "            en_band.append(entropy(S[ind].ravel()))\n",
    "            \n",
    "\n",
    "        return en_band\n",
    "    \n",
    "    \n",
    "    def getClutter_SE(self, wlevels=3, wght_chrom=0.0625) -> float:\n",
    "        \"\"\"\n",
    "        \n",
    "        Computes the subband entropy measure of visual clutter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        wlevels : the number of scales (optional, default 3)\n",
    "        wght_chrom : the weight on chrominance (optional, default 0.0625)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        clutter_se : float, the subband entropy clutter of the image.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This measure (Subband Entropy) of visual clutter is based on the notion\n",
    "        that clutter is related to the number of bits required for subband\n",
    "        (wavelet) image coding.\n",
    "        \n",
    "        Examples\n",
    "        --------\n",
    "        >>> clt = Clutter('test.jpg')\n",
    "        >>> clutter_se = clt.getClutter_SE(wlevels=3, wght_chrom=0.0625)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        wor = 4\n",
    "        # luminance channel:\n",
    "        en_band = self.band_entropy(self.L, wlevels, wor)\n",
    "        clutter_se = np.mean(en_band)\n",
    "\n",
    "        if self.d == 1:\n",
    "            return clutter_se\n",
    "\n",
    "        # chrominance channels:\n",
    "        for jj in [self.a, self.b]:\n",
    "            if np.max(jj)-np.min(jj) < 0.008:\n",
    "                jj = np.zeros_like(jj)\n",
    "\n",
    "            en_band = self.band_entropy(jj, wlevels, wor)\n",
    "            clutter_se = clutter_se + wght_chrom * np.mean(en_band)\n",
    "\n",
    "        clutter_se = clutter_se/(1 + 2 * wght_chrom)\n",
    "        return clutter_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99eed8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clt = Clutter('test.jpg', numlevels=3, contrast_filt_sigma=1, contrast_pool_sigma=3, color_pool_sigma=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8ed487",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_clutter = clt.colorClutter(color_pix=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03fd030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_clutter = clt.contrastClutter(contrast_pix=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc04b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orientation_clutter = clt.orientationClutter(orient_pix=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87cdb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_clutter, contrast_clutter, orientation_clutter = clt.computeClutter(color_pix=1, contrast_pix=1, orient_pix=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7883965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clutter_scalar_fc, clutter_map_fc = clt.getClutter_FC(p=1, pix=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20d5d1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6153260859434835"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clt.getClutter_SE(wlevels=3, wght_chrom=0.0625)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
